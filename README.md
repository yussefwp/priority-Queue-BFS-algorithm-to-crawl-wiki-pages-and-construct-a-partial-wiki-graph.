# priority-Queue-BFS-algorithm-to-crawl-wiki-pages-and-construct-a-partial-wiki-graph.
Description of a programming assignment is not a linear narrative and may require multiplereadings before things start to make sense.  You are encouraged to start working on the assignmentas soon as possible.  If you start a day or two before the deadline, it is highly unlikely that you willbe able to come with correct and efficient programs.You are encouraged to consult instructor/Teaching Assistants for any questions/clarificationsregarding the assignment.  Your programs must be in Java, preferably Java 8.1.  You should notuse any external libraries.  Any libraries that you import must start withjava.For this PA, youmay work in teams of 2.  It is your responsibility to find a team member.If you can not find a team member, then you must work on your own.Only one submission perteam.In this programming assignment you will•Implement a priority Queue (Max Heap) to store Strings (along with their priorities).•Implement (variants of) BFS algorithm to crawl wiki pages and construct a partial wiki graph.You will design following classes:1.  PriorityQ2.  WikiCrawlerAll  your  classes  must  be  in  thedefault  package(even  though  it  is  not  a  good  programmingpractice).1    PriorityQRead Section 2.5 from the text to refresh your understandings about priority queues.  A priorityqueue storesdata itemsalong with theirpriority(often calledkeyof the item).  If the data item isvand its priority isp, then the tuple〈v, p〉is store in the priority queue.  For any such tuplet=〈v, p〉we say thatkey(t) =pandval(t) =v.  In this PA, you will implement a priority queue using maxheap, which is implemented as an arrayA, where the max heap property is that for everyi(arrayindex starts at 1),key(A[i])≥key(A[2i]) andkey(A[i])≥key(A[2i+ 1])In this assignment, the data items arestrings, where each string has an associated priority.1
Your Objective.Implement a classPriorityQwith the following methods1.PriorityQ():  constructs an empty priority queue.2.add(String s, int p):  Adds a stringswith prioritypto the priority queue.3.returnMax():  returns a string whose priority is maximum.4.extractMax():  returns a string whose priority is maximum and removes it from the priorityqueue.5.remove(int i):  removes the element from the priority queue whose array index isi.6.decrementPriority(int i, into).  Decrements the priority of theith element byk.7.priorityArray():  returns an arrayBwith the following property:B[i] = key(A[i])foralliin the arrayAused to implement the priority queue.The max heap property must be maintained after each of the above operation.  Methods 3–7 havea pre-condition that priority queue is non-empty.2    BFS and Web GraphIn this problem, you will generate a exploration graph. Recall that given a graph, the BFS algorithmis as follows.1. Input: Directed Graph G = (V, E)Root of exploration: root2. Initilize a FIFO queue Q and a list Discovered3. Add root to Q and Discovered4. while Q is not empty do5.    Extract vertex v from the head of the Q6.    For every (v, v’) in E do7.       if v’ is not in Discovered then8.          add v’ to Q and DiscoveredIf you output the vertices in theDiscoveredlist, then that will produce the BFS traversal of theinput graph starting from the root.We can the model web as a directed graph.  Every web page is a vertex of the graph.  We puta directed edge from a pagepto a pageq, if pagepcontains a link to pageq.  Given a root (wewill call it seed URL of the web graph),  we can explore this web graph using BFS algorithm aspresented above.2.1    Crawling and Constructing Web GraphOne of the first tasks of a web search engine is tocrawlthe web to collect material from all webpages.  The crawl may be performed in a BFS fashion.  While doing this, the the search engine willalso construct theweb graph.  The structure of this graph is critical in determining the pages thatare relevant to a query.2
As  part  of  this  assignment  you  will  write  a  program  to  docrawlof  the  web  and  constructweb  graph.   However,  web  graph  is  too  large,  and  you  do  not  have  computational  resources  toconstruct the entire web graph (unless you own a super computer).  Thus your program will crawland  construct  the  web  graph  over  200  pages.   Furthermore,  Your  program  will  crawl  onlyWikipages.Relevance  of  a  page.Before  we  proceed,  let  us  discuss  the  notion  offocused crawling.   Incertain application, you would like to visit only the web pages that are related to certain topic ofinterest.  For example,  if I would like to collect only the web pages that are related to theIowaState University, then any page that is not aboutIowa State Universityis not of interest to me.Furthermore, my goal is not collecteveryweb page about Iowa State University.  My goal wouldbe to collect top, say 200, pages that aremost relevantto Iowa State University.  For this, we needa way to determine the relevancy of a page to the topic of my interest.LetTbe a set of strings that describe a topic.  For example, for my topic “Iowa State Univer-sity”, the strings that describe this topic could be “Iowa State University, ISU, Cyclones, Ames,Atanasoff”.  Given a web addressa, letpage(a) denote the contents of the page at addressa.  Givena  web  addressaand  a  strings,  letf(s, a)  denote  the  number  of  times  the  stringsappears  inpage(a).This is case sensitive.  For aT, the relevancy of a web addressato the topicTis definedas”:Relevance(T, a) =∑s∈Tf(s, a)Your Objective.Implement a classWikiCrawlerwith the following methods.1.  The constructorWikiCrawler(String seed,int max,String[] topics,String output)}where(a)seed:  related address of seed URL (within wiki domain)(b)max:  maximum number of pages to consider(c)topics:  array of strings representing keywords in a topic-list(d)output:  string representing the filename where theweb graphover discovered pages arewritten.2.ArrayList<String> extractLinks(string document):  the method takes as input a docu-ment representing an entire HTML document.  It returns a list of strings consisting of linksfrom thedocument.  You can assume that thedocumentis HTML from some wiki page.  Themethod must(a)  extract only relative addresses of wiki links, i.e., only links that are of the form/wiki/XXXX(b)  only extract links that appear after the first occurrence of the html tag<p>(or<P>)3
(c)  Must not extract any wiki link that contain characters such as “#” or “:”(d)  The order in which the links in the returned array list must be exactly the same orderin which they appear in thedocument3.crawl(boolean focused): crawls/explores the web pages starting from the seed URL. Crawlthefirstmaxnumber of pages (including theseedpage), that contains every keywords in theTopicslist (ifTopicslist is empty then this condition is vacuously considered true), and areexplored starting from theseed.(a)  iffocusedis false then explore in a BFS fashion(b)  iffocusedis  true  then  for  every  pagea,  compute  theRelevance(T, a),  and  duringexploration, instead of adding the pages in the FIFO queue,•add the pages and their corresponding relevance (to topic) to priority queue.  Thepriority of a page is its relevance;•extract elements from the queue usingextractMax.After the crawl is done, the edges explored in the crawl method should be written to theoutputfile.2.2    Guidelines, Clarifications and Suggestions for Part 21.  Review the enclosed sample outputs to better understand the result of crawling.2.  Here is an example.  Suppose that there following pages.A, B, C, D, E, F, G, H, I, J.  PageAhas links toB, CandDappearing in that order.  PageChas links toE, F,BandD.  PageDhas links toG, HandA.  PageBhas links toIandJ.  PageEhas a link to pageA.  Noneof the other pages have any links.  Whenfocusedis false, theseedURL isA, andmaxis 6,the the constructed graph will have the following edges:AtoB,AtoC,AtoD,BtoI,BtoJ,DtoA,CtoB, andCtoD.  Thus the output file looks like6A BA CA DB IB JD AC BC DFirst line denotes the number of web pages discovered.  Each subsequent line lists an edge.3.  The seed url is specified asrelative address; for example/wiki/IowaStateUniversitynotashttps://en.wikipedia.org/wiki/IowaStateUniversity.4
4.  Extract only links from “actual text component”.  A typical wiki page will have a panel onthe left hand side that contains some navigational links.  Your program should not extractany of such links.  Wiki pages have a nice structure that enables us to do this easily.  For thepurpose of this PA, the “actual text content” of the page starts immediately after the firstoccurrence of the html tag<p>.  So, your program should extract links (pages) that appearafter the first occurrence of<p>.  Your program must extract the linksin the order theyappear in the page, and whenfocusedis false place them in queue in that order.5.  While computing the relevance of a page totopics, inre the content that appears before thefirst occurrence of<p>.6.  The graph constructedshould not have self loops nor it should have multiple edgesbetween same pair of nodes.(even though a page might refer to itself or refer to anotherpage multiple times).7.  If you wish you may take a look at the graph our crawler constructed (With/wiki/Complexitytheoryas root and 100 as maximum number of pages, and empty list as topics) This graph has 847edges.  The graph is attached along with the assignment (namedwikiCC.txt).  Crawl Date:Sep 21, 9:50AM. Please note the format of the file.  Before you ask any questions such as “whydoes this graph contain only 847 edges”, “the graph I constructed has way more edges/pages”,please understand Item 2.8.  Crawlers place considerable load on the servers (from which they are requesting a page).  Ifyour program continuously sends request to a server, you may be denied access.  Thus yourprogram  must  adhere  to  the  following  politeness  policy:  Wait  for  at  least  3  seconds  afterevery 20 requests.If you do not adhere to this policy you will receive ZERO credit.Please useThread.sleep()for waiting.Absolutely no exceptions to this policy.9.Your classWikiCrawlermust declare astatic, finalglobal variable namedBASEURLwith valuehttps://en.wikipedia.organd use in conjunction with links of the form/wiki/XXXXwhen  sending  a  request  fetch  a  page.   Otherwise,  you  will  receiveZERO credit.  No exceptions.For example, your code to fetch page athttps://en.wikipedia.org/wiki/PhysicscouldbeURL url = new URL(BASE_URL+"/wiki/Physics");InputStream is = url.openStream();BufferedReader br = new BufferedReader(new InputStreamReader(is));10.  Your program should not use any external packages to parse html pages, to extract links fromhtml pages and to extract text from html pages.  You can only use the packagejava.netforthis.11.  If most of you work in the last hour and start sending requests to wiki pages, it is quite possiblethat wiki may slow down, or deny service to any request from the domainiastate.edu.  Thiscan not be an excuse for late submissions or extending the deadline.  You are advised to startworking on PA as soon as possible.5
12.  Your program must strictly adhere to the specifications.  Class, method names, type and orderof parameters to the methods, and the return types of methods must be exactly as specified.Any deviation from the specification will be considered incorrect programming and lead todeduction of significant portion of points.13.  Your grade depends on correctness, efficiency and adherence to specifications.14.  Any questions posted on Piazza within 24 hours prior to the submission due time may notbe answered.15.  Your program must not use any of Java’s in built classes for priority queues.
